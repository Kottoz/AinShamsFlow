<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ainshamsflow.losses API documentation</title>
<meta name="description" content="Losses Module â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ainshamsflow.losses</code></h1>
</header>
<section id="section-intro">
<p>Losses Module.</p>
<p>In this Module we provide our loss functions for a variety of
use cases like Mean Squared Error or Cross Entropy loss.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Losses Module.

In this Module we provide our loss functions for a variety of
use cases like Mean Squared Error or Cross Entropy loss.
&#34;&#34;&#34;

import numpy as np
from ainshamsflow.utils.asf_errors import BaseClassError, NameNotFoundError
from ainshamsflow.utils.utils import true_one_hot

__pdoc__ = dict()

for loss_n in [&#39;Loss&#39;, &#39;MSE&#39;, &#39;MAE&#39;, &#39;MAPE&#39;, &#39;HuberLoss&#39;, &#39;LogLossLinear&#39;, &#39;LogLossSigmoid&#39;,
              &#39;PerceptronCriterion&#39;, &#39;SvmHingeLoss&#39;, &#39;BinaryCrossentropy&#39;, &#39;CategoricalCrossentropy&#39;,
              &#39;SparseCategoricalCrossentropy&#39;]:
    __pdoc__[loss_n + &#39;.__call__&#39;] = True


def get(loss_name):
    &#34;&#34;&#34;Get any Loss in this module by name&#34;&#34;&#34;
    losses = [MSE, MAE, MAPE, HuberLoss, LogLossLinear, LogLossSigmoid,
              PerceptronCriterion, SvmHingeLoss, BinaryCrossentropy, CategoricalCrossentropy,
              SparseCategoricalCrossentropy]
    for loss in losses:
        if loss.__name__.lower() == loss_name.lower():
            return loss()
    raise NameNotFoundError(loss_name, __name__)


class Loss:
    &#34;&#34;&#34;Loss Base Class.

    To create a new Loss Function, create a class that inherits
    from this class.
    You then have to add any parameters in your constructor
    and redefine the __call__() and diff() methods.
    Note: all loss functions can be used as metrics.
    &#34;&#34;&#34;

    def __call__(self, y_pred, y_true):
        &#34;&#34;&#34;Use the Loss function to get the loss Value.&#34;&#34;&#34;
        raise BaseClassError

    def diff(self, y_pred, y_true):
        &#34;&#34;&#34;Get the derivative of the loss function.&#34;&#34;&#34;
        raise BaseClassError


class MSE(Loss):
    &#34;&#34;&#34;Mean squared error loss function.

    Computes the mean squared error between labels and predictions.

    After computing the squared distance between the inputs, the mean value over
    the last dimension is returned.
    `loss = mean(square(y_true - y_pred), axis=-1)`

    Standalone usage:

    ```python
    &gt;&gt;&gt; y_true = np.random.randint(0, 2, size=(2, 3))
    &gt;&gt;&gt; y_pred = np.random.random(size=(2, 3))
    &gt;&gt;&gt; loss = asf.losses.MSE()
    &gt;&gt;&gt; loss(y_pred,y_true)
    ```
    Args:
        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.
        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.
    Returns:
        error_values: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`.
    &#34;&#34;&#34;
    __name__ = &#39;MSE&#39;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return np.sum(np.square(y_pred - y_true)) / (2 * m)

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        return (y_pred - y_true) / m


class MAE(Loss):
    &#34;&#34;&#34;Mean Absolute Error.

    Computes the mean absolute error between labels and predictions.

    `loss = mean(abs(y_true - y_pred), axis=-1)`

    Standalone usage:
    ```python
    &gt;&gt;&gt; y_true = np.random.randint(0, 2, size=(2, 3))
    &gt;&gt;&gt; y_pred = np.random.random(size=(2, 3))
    &gt;&gt;&gt; loss = asf.losses.MAE()
    &gt;&gt;&gt; loss(y_pred,y_true)
    ```
     Args:
        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.
        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.
     Returns:
        Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`.
    &#34;&#34;&#34;
    __name__ = &#39;MAE&#39;

    def __call__(self, y_pred, y_true):
        m = y_true.shape[0]
        return np.sum(np.abs(y_pred - y_true)) / m

    def diff(self, y_pred, y_true):
        m = y_true.shape[0]
        return np.sign(y_pred - y_true) / m


class HuberLoss(Loss):
    &#34;&#34;&#34;Huber loss error.

    Computes the Huber loss between `y_true` and `y_pred`.

    For each value x in `error = y_true - y_pred`:
    ```
    loss = 0.5 * x^2                  if |x| &lt;= d
    loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d
    ```
    where d is `delta`. See: https://en.wikipedia.org/wiki/Huber_loss
    Standalone usage:
    ```python
    &gt;&gt;&gt; y_true = [[0, 1], [0, 0]]
    &gt;&gt;&gt; y_pred = [[0.6, 0.4], [0.4, 0.6]]
    &gt;&gt;&gt; # Using &#39;auto&#39;/&#39;sum_over_batch_size&#39; reduction type.
    &gt;&gt;&gt; loss = asf.losses.HuberLoss()
    &gt;&gt;&gt; loss(y_true, y_pred)
    0.155
    ```
    &#34;&#34;&#34;
    __name__ = &#39;HuberLoss&#39;

    def __init__(self, delta=1.0):
        self.delta = delta

    def __call__(self, y_pred, y_true):
        return np.where(np.abs(y_pred - y_true) &lt;= self.delta, 0.5 * np.square(y_pred - y_true),
                        (self.delta * np.abs(y_pred - y_true)) - 0.5 * np.square(self.delta))

    def diff(self, y_pred, y_true):
        return np.where(np.abs(y_pred - y_true) &lt;= self.delta, y_pred - y_true, self.delta * np.sign(y_pred - y_true))


class MAPE(Loss):
    &#34;&#34;&#34;Mean absolute percentage error.

    Computes the mean absolute percentage error between `y_true` and `y_pred`.

    `loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)`
    Standalone usage:
    ```python
    &gt;&gt;&gt; y_true = np.random.rand(2, 3)
    &gt;&gt;&gt; y_pred = np.random.random(size=(2, 3))
    &gt;&gt;&gt; loss = asf.losses.MAPE()
    &gt;&gt;&gt; loss(y_pred,y_true)
    ```
    Args:
        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.
        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.
    Returns:
        Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`.
    &#34;&#34;&#34;
    __name__ = &#39;MAPE&#39;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return np.sum(np.abs(y_pred - y_true) / y_true) / m

    def diff(self, y_pred, y_true):
        m = y_true.shape[0]
        return np.where(y_pred &gt; y_true, 1 / (m * y_true), -1 / (m * y_true))


class LogLossLinear(Loss):
    &#34;&#34;&#34;Logistic loss in case of identity(linear) activation function.

    Computes the logistic regression loss between y_pred and y_true.

    This class is used only in case of linear activation function and
    is provided by y_pred and y_true
    Stand alone usage:
    ```python
    &gt;&gt;&gt; y_pred=2
    &gt;&gt;&gt; y_true=2.4
    &gt;&gt;&gt; loss=asf.losses.LogLossLinear()
    &gt;&gt;&gt; loss(2.4,2)
    0.008196
    ```
    Returns:
        the logistic loss values in case of linear regression
    &#34;&#34;&#34;
    __name__ = &#34;LogLossLinear&#34;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return np.sum(np.log(1 + np.exp(-y_true * y_pred))) / m

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        return -y_true * np.exp(-y_true * y_pred) / (1 + np.exp(-y_true * y_pred)) / m


class LogLossSigmoid(Loss):
    &#34;&#34;&#34;Logistic loss in case of sigmoid activation functions.

       Computes the logistic regression loss between y_pred and y_true.

        This class is used only in case of linear activation function and
        is provided by y_pred and y_true
        Standalone usage:
        ```python
        &gt;&gt;&gt; y_pred=2
        &gt;&gt;&gt; y_true=2.4
        &gt;&gt;&gt; loss=asf.losses.LogLossSigmoid()
        &gt;&gt;&gt; loss(y_pred,y_true)
        ```
        Returns:
            the logistic loss values in case of linear regression
    &#34;&#34;&#34;
    __name__ = &#34;LogLossSigmoid.&#34;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return -np.sum(np.log(np.abs(y_true / 2 - 0.5 + y_pred))) / m

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        x = y_true / 2 - 0.5 + y_pred
        return -np.sign(x) / np.abs(x) / m


class PerceptronCriterion(Loss):
    &#34;&#34;&#34;Bipolar perceptron criterion loss class.
       Standalone usage:
       ```python
       &gt;&gt;&gt; y_pred=2
       &gt;&gt;&gt; y_true=2.4
       &gt;&gt;&gt; loss=asf.losses.PerceptronCriterion()
       &gt;&gt;&gt; loss(y_pred,y_true)
       0
       &gt;&gt;&gt; loss.(-y_pred,y_true)
       4.8
       ```
       Returns:
           0 if both numbers are positives or negatives
           otherwise ,returns their product
    &#34;&#34;&#34;
    __name__ = &#39;PerceptronCriterion&#39;

    def __call__(self, y_pred, y_true):
        return np.maximum(0, -y_true * y_pred)

    def diff(self, y_pred, y_true):
        return np.where(y_true * y_pred &lt;= 0, -y_true, 0)


class SvmHingeLoss(Loss):
    &#34;&#34;&#34;SVM hinge criterion loss.

    Stand alone usage:
    ```python
    &gt;&gt;&gt; y_pred=2
    &gt;&gt;&gt; y_true=2.4
    &gt;&gt;&gt; loss=asf.losses.SvmHingeLoss()
    &gt;&gt;&gt; loss(y_pred,y_true)
    0
    &gt;&gt;&gt; loss(-y_pred,y_true)
    5.8
    ```
     Returns:
        0 if product of the two numbers is greater than 1
        otherwise ,returns 1-their product
    &#34;&#34;&#34;
    __name__ = &#39;SvmHingeLoss&#39;

    def __call__(self, y_pred, y_true):
        return np.maximum(0, 1 - y_true * y_pred)

    def diff(self, y_pred, y_true):
        return np.where(y_true * y_pred &lt;= 1, -y_true, 0)


class BinaryCrossentropy(Loss):
    &#34;&#34;&#34;Binary cross entropy Loss.

    Computes the cross-entropy loss between true labels and predicted labels.

    Use this cross-entropy loss when there are only two label classes (assumed to
    be 0 and 1). For each example, there should be a single floating-point value
    per prediction.
    Standalone usage:
    ```python
    &gt;&gt;&gt; y_true = [[1], [0]]
    &gt;&gt;&gt; y_pred = [[0.6, 0.4], [0.4, 0.6]]
    &gt;&gt;&gt; loss = asf.losses.BinaryCrossentropy()
    &gt;&gt;&gt; loss(y_pred,y_true)
    0.815
    &#34;&#34;&#34;

    __name__ = &#39;BinaryCrossentropy&#39;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return -np.sum(np.where(y_true, np.log(y_pred), np.log(1 - y_pred))) / m

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        return (y_pred - y_true) / m


class CategoricalCrossentropy(Loss):
    &#34;&#34;&#34;Computes the crossentropy loss between the labels and predictions.

      Use this crossentropy loss function when there are two or more label classes.
      We expect labels to be provided in a `one_hot` representation. If you want to
      provide labels as integers, please use `SparseCategoricalCrossentropy` loss.
      There should be `# classes` floating point values per feature.
      Standalone usage:
      ```python
      &gt;&gt;&gt; y_true = [[0, 1, 0], [0, 0, 1]]
      &gt;&gt;&gt; y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
      &gt;&gt;&gt; loss = asf.losses.CategoricalCrossentropy()
      &gt;&gt;&gt; loss(y_pred, y_true).
      1.177
      ```
    &#34;&#34;&#34;
    __name__ = &#39;CategoricalCrossentropy&#39;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return -np.sum(np.log(np.max(y_true * y_pred, axis=1) + 1e-6)) / m

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        return (y_pred - y_true) / m


class SparseCategoricalCrossentropy(Loss):
    &#34;&#34;&#34;Computes the crossentropy loss between the labels and predictions.

      Use this crossentropy loss function when there are two or more label classes.
      We expect labels to be provided as integers. If you want to provide labels
      using `one-hot` representation, please use `CategoricalCrossentropy` loss.
      There should be `# classes` floating point values per feature for `y_pred`
      and a single floating point value per feature for `y_true`

      Standalone usage:
      ```python
      &gt;&gt;&gt;  y_true = [[1], [2]]
      &gt;&gt;&gt;  y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
      &gt;&gt;&gt;  loss = asf.losses.SparseCategoricalCrossentropy)
      &gt;&gt;&gt;  loss(y_pred,y_true)
      1.177
      ```
    &#34;&#34;&#34;
    __name__ = &#39;SparseCategoricalCrossentropy&#39;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        n_c = y_pred.shape[-1]
        y_true = true_one_hot(y_true, n_c)
        return -np.sum(np.log(np.max(y_true * y_pred, axis=1))) / m

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        n_c = y_pred.shape[-1]
        y_true = true_one_hot(y_true, n_c)
        return (y_pred - y_true) / m</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ainshamsflow.losses.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>loss_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Get any Loss in this module by name</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(loss_name):
    &#34;&#34;&#34;Get any Loss in this module by name&#34;&#34;&#34;
    losses = [MSE, MAE, MAPE, HuberLoss, LogLossLinear, LogLossSigmoid,
              PerceptronCriterion, SvmHingeLoss, BinaryCrossentropy, CategoricalCrossentropy,
              SparseCategoricalCrossentropy]
    for loss in losses:
        if loss.__name__.lower() == loss_name.lower():
            return loss()
    raise NameNotFoundError(loss_name, __name__)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ainshamsflow.losses.BinaryCrossentropy"><code class="flex name class">
<span>class <span class="ident">BinaryCrossentropy</span></span>
</code></dt>
<dd>
<div class="desc"><p>Binary cross entropy Loss.</p>
<p>Computes the cross-entropy loss between true labels and predicted labels.</p>
<p>Use this cross-entropy loss when there are only two label classes (assumed to
be 0 and 1). For each example, there should be a single floating-point value
per prediction.
Standalone usage:</p>
<pre><code class="python">```python-repl
&gt;&gt;&gt; y_true = [[1], [0]]
&gt;&gt;&gt; y_pred = [[0.6, 0.4], [0.4, 0.6]]
&gt;&gt;&gt; loss = asf.losses.BinaryCrossentropy()
&gt;&gt;&gt; loss(y_pred,y_true)
0.815
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BinaryCrossentropy(Loss):
    &#34;&#34;&#34;Binary cross entropy Loss.

    Computes the cross-entropy loss between true labels and predicted labels.

    Use this cross-entropy loss when there are only two label classes (assumed to
    be 0 and 1). For each example, there should be a single floating-point value
    per prediction.
    Standalone usage:
    ```python
    &gt;&gt;&gt; y_true = [[1], [0]]
    &gt;&gt;&gt; y_pred = [[0.6, 0.4], [0.4, 0.6]]
    &gt;&gt;&gt; loss = asf.losses.BinaryCrossentropy()
    &gt;&gt;&gt; loss(y_pred,y_true)
    0.815
    &#34;&#34;&#34;

    __name__ = &#39;BinaryCrossentropy&#39;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return -np.sum(np.where(y_true, np.log(y_pred), np.log(1 - y_pred))) / m

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        return (y_pred - y_true) / m</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ainshamsflow.losses.CategoricalCrossentropy"><code class="flex name class">
<span>class <span class="ident">CategoricalCrossentropy</span></span>
</code></dt>
<dd>
<div class="desc"><p>Computes the crossentropy loss between the labels and predictions.</p>
<p>Use this crossentropy loss function when there are two or more label classes.
We expect labels to be provided in a <code>one_hot</code> representation. If you want to
provide labels as integers, please use <code><a title="ainshamsflow.losses.SparseCategoricalCrossentropy" href="#ainshamsflow.losses.SparseCategoricalCrossentropy">SparseCategoricalCrossentropy</a></code> loss.
There should be <code># classes</code> floating point values per feature.
Standalone usage:</p>
<pre><code class="python">&gt;&gt;&gt; y_true = [[0, 1, 0], [0, 0, 1]]
&gt;&gt;&gt; y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
&gt;&gt;&gt; loss = asf.losses.CategoricalCrossentropy()
&gt;&gt;&gt; loss(y_pred, y_true).
1.177
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CategoricalCrossentropy(Loss):
    &#34;&#34;&#34;Computes the crossentropy loss between the labels and predictions.

      Use this crossentropy loss function when there are two or more label classes.
      We expect labels to be provided in a `one_hot` representation. If you want to
      provide labels as integers, please use `SparseCategoricalCrossentropy` loss.
      There should be `# classes` floating point values per feature.
      Standalone usage:
      ```python
      &gt;&gt;&gt; y_true = [[0, 1, 0], [0, 0, 1]]
      &gt;&gt;&gt; y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
      &gt;&gt;&gt; loss = asf.losses.CategoricalCrossentropy()
      &gt;&gt;&gt; loss(y_pred, y_true).
      1.177
      ```
    &#34;&#34;&#34;
    __name__ = &#39;CategoricalCrossentropy&#39;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return -np.sum(np.log(np.max(y_true * y_pred, axis=1) + 1e-6)) / m

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        return (y_pred - y_true) / m</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ainshamsflow.losses.HuberLoss"><code class="flex name class">
<span>class <span class="ident">HuberLoss</span></span>
<span>(</span><span>delta=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Huber loss error.</p>
<p>Computes the Huber loss between <code>y_true</code> and <code>y_pred</code>.</p>
<p>For each value x in <code>error = y_true - y_pred</code>:</p>
<pre><code>loss = 0.5 * x^2                  if |x| &lt;= d
loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d
</code></pre>
<p>where d is <code>delta</code>. See: <a href="https://en.wikipedia.org/wiki/Huber_loss">https://en.wikipedia.org/wiki/Huber_loss</a>
Standalone usage:</p>
<pre><code class="python">&gt;&gt;&gt; y_true = [[0, 1], [0, 0]]
&gt;&gt;&gt; y_pred = [[0.6, 0.4], [0.4, 0.6]]
&gt;&gt;&gt; # Using 'auto'/'sum_over_batch_size' reduction type.
&gt;&gt;&gt; loss = asf.losses.HuberLoss()
&gt;&gt;&gt; loss(y_true, y_pred)
0.155
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HuberLoss(Loss):
    &#34;&#34;&#34;Huber loss error.

    Computes the Huber loss between `y_true` and `y_pred`.

    For each value x in `error = y_true - y_pred`:
    ```
    loss = 0.5 * x^2                  if |x| &lt;= d
    loss = 0.5 * d^2 + d * (|x| - d)  if |x| &gt; d
    ```
    where d is `delta`. See: https://en.wikipedia.org/wiki/Huber_loss
    Standalone usage:
    ```python
    &gt;&gt;&gt; y_true = [[0, 1], [0, 0]]
    &gt;&gt;&gt; y_pred = [[0.6, 0.4], [0.4, 0.6]]
    &gt;&gt;&gt; # Using &#39;auto&#39;/&#39;sum_over_batch_size&#39; reduction type.
    &gt;&gt;&gt; loss = asf.losses.HuberLoss()
    &gt;&gt;&gt; loss(y_true, y_pred)
    0.155
    ```
    &#34;&#34;&#34;
    __name__ = &#39;HuberLoss&#39;

    def __init__(self, delta=1.0):
        self.delta = delta

    def __call__(self, y_pred, y_true):
        return np.where(np.abs(y_pred - y_true) &lt;= self.delta, 0.5 * np.square(y_pred - y_true),
                        (self.delta * np.abs(y_pred - y_true)) - 0.5 * np.square(self.delta))

    def diff(self, y_pred, y_true):
        return np.where(np.abs(y_pred - y_true) &lt;= self.delta, y_pred - y_true, self.delta * np.sign(y_pred - y_true))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ainshamsflow.losses.LogLossLinear"><code class="flex name class">
<span>class <span class="ident">LogLossLinear</span></span>
</code></dt>
<dd>
<div class="desc"><p>Logistic loss in case of identity(linear) activation function.</p>
<p>Computes the logistic regression loss between y_pred and y_true.</p>
<p>This class is used only in case of linear activation function and
is provided by y_pred and y_true
Stand alone usage:</p>
<pre><code class="python">&gt;&gt;&gt; y_pred=2
&gt;&gt;&gt; y_true=2.4
&gt;&gt;&gt; loss=asf.losses.LogLossLinear()
&gt;&gt;&gt; loss(2.4,2)
0.008196
</code></pre>
<h2 id="returns">Returns</h2>
<p>the logistic loss values in case of linear regression</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogLossLinear(Loss):
    &#34;&#34;&#34;Logistic loss in case of identity(linear) activation function.

    Computes the logistic regression loss between y_pred and y_true.

    This class is used only in case of linear activation function and
    is provided by y_pred and y_true
    Stand alone usage:
    ```python
    &gt;&gt;&gt; y_pred=2
    &gt;&gt;&gt; y_true=2.4
    &gt;&gt;&gt; loss=asf.losses.LogLossLinear()
    &gt;&gt;&gt; loss(2.4,2)
    0.008196
    ```
    Returns:
        the logistic loss values in case of linear regression
    &#34;&#34;&#34;
    __name__ = &#34;LogLossLinear&#34;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return np.sum(np.log(1 + np.exp(-y_true * y_pred))) / m

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        return -y_true * np.exp(-y_true * y_pred) / (1 + np.exp(-y_true * y_pred)) / m</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ainshamsflow.losses.LogLossSigmoid"><code class="flex name class">
<span>class <span class="ident">LogLossSigmoid</span></span>
</code></dt>
<dd>
<div class="desc"><p>Logistic loss in case of sigmoid activation functions.</p>
<p>Computes the logistic regression loss between y_pred and y_true.</p>
<p>This class is used only in case of linear activation function and
is provided by y_pred and y_true
Standalone usage:
```python</p>
<blockquote>
<blockquote>
<blockquote>
<p>y_pred=2
y_true=2.4
loss=asf.losses.LogLossSigmoid()
loss(y_pred,y_true)
```
Returns:
the logistic loss values in case of linear regression</p>
</blockquote>
</blockquote>
</blockquote></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogLossSigmoid(Loss):
    &#34;&#34;&#34;Logistic loss in case of sigmoid activation functions.

       Computes the logistic regression loss between y_pred and y_true.

        This class is used only in case of linear activation function and
        is provided by y_pred and y_true
        Standalone usage:
        ```python
        &gt;&gt;&gt; y_pred=2
        &gt;&gt;&gt; y_true=2.4
        &gt;&gt;&gt; loss=asf.losses.LogLossSigmoid()
        &gt;&gt;&gt; loss(y_pred,y_true)
        ```
        Returns:
            the logistic loss values in case of linear regression
    &#34;&#34;&#34;
    __name__ = &#34;LogLossSigmoid.&#34;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return -np.sum(np.log(np.abs(y_true / 2 - 0.5 + y_pred))) / m

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        x = y_true / 2 - 0.5 + y_pred
        return -np.sign(x) / np.abs(x) / m</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ainshamsflow.losses.Loss"><code class="flex name class">
<span>class <span class="ident">Loss</span></span>
</code></dt>
<dd>
<div class="desc"><p>Loss Base Class.</p>
<p>To create a new Loss Function, create a class that inherits
from this class.
You then have to add any parameters in your constructor
and redefine the <strong>call</strong>() and diff() methods.
Note: all loss functions can be used as metrics.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Loss:
    &#34;&#34;&#34;Loss Base Class.

    To create a new Loss Function, create a class that inherits
    from this class.
    You then have to add any parameters in your constructor
    and redefine the __call__() and diff() methods.
    Note: all loss functions can be used as metrics.
    &#34;&#34;&#34;

    def __call__(self, y_pred, y_true):
        &#34;&#34;&#34;Use the Loss function to get the loss Value.&#34;&#34;&#34;
        raise BaseClassError

    def diff(self, y_pred, y_true):
        &#34;&#34;&#34;Get the derivative of the loss function.&#34;&#34;&#34;
        raise BaseClassError</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.BinaryCrossentropy" href="#ainshamsflow.losses.BinaryCrossentropy">BinaryCrossentropy</a></li>
<li><a title="ainshamsflow.losses.CategoricalCrossentropy" href="#ainshamsflow.losses.CategoricalCrossentropy">CategoricalCrossentropy</a></li>
<li><a title="ainshamsflow.losses.HuberLoss" href="#ainshamsflow.losses.HuberLoss">HuberLoss</a></li>
<li><a title="ainshamsflow.losses.LogLossLinear" href="#ainshamsflow.losses.LogLossLinear">LogLossLinear</a></li>
<li><a title="ainshamsflow.losses.LogLossSigmoid" href="#ainshamsflow.losses.LogLossSigmoid">LogLossSigmoid</a></li>
<li><a title="ainshamsflow.losses.MAE" href="#ainshamsflow.losses.MAE">MAE</a></li>
<li><a title="ainshamsflow.losses.MAPE" href="#ainshamsflow.losses.MAPE">MAPE</a></li>
<li><a title="ainshamsflow.losses.MSE" href="#ainshamsflow.losses.MSE">MSE</a></li>
<li><a title="ainshamsflow.losses.PerceptronCriterion" href="#ainshamsflow.losses.PerceptronCriterion">PerceptronCriterion</a></li>
<li><a title="ainshamsflow.losses.SparseCategoricalCrossentropy" href="#ainshamsflow.losses.SparseCategoricalCrossentropy">SparseCategoricalCrossentropy</a></li>
<li><a title="ainshamsflow.losses.SvmHingeLoss" href="#ainshamsflow.losses.SvmHingeLoss">SvmHingeLoss</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ainshamsflow.losses.Loss.__call__"><code class="name flex">
<span>def <span class="ident">__call__</span></span>(<span>self, y_pred, y_true)</span>
</code></dt>
<dd>
<div class="desc"><p>Use the Loss function to get the loss Value.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __call__(self, y_pred, y_true):
    &#34;&#34;&#34;Use the Loss function to get the loss Value.&#34;&#34;&#34;
    raise BaseClassError</code></pre>
</details>
</dd>
<dt id="ainshamsflow.losses.Loss.diff"><code class="name flex">
<span>def <span class="ident">diff</span></span>(<span>self, y_pred, y_true)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the derivative of the loss function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def diff(self, y_pred, y_true):
    &#34;&#34;&#34;Get the derivative of the loss function.&#34;&#34;&#34;
    raise BaseClassError</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ainshamsflow.losses.MAE"><code class="flex name class">
<span>class <span class="ident">MAE</span></span>
</code></dt>
<dd>
<div class="desc"><p>Mean Absolute Error.</p>
<p>Computes the mean absolute error between labels and predictions.</p>
<p><code>loss = mean(abs(y_true - y_pred), axis=-1)</code></p>
<p>Standalone usage:</p>
<pre><code class="python">&gt;&gt;&gt; y_true = np.random.randint(0, 2, size=(2, 3))
&gt;&gt;&gt; y_pred = np.random.random(size=(2, 3))
&gt;&gt;&gt; loss = asf.losses.MAE()
&gt;&gt;&gt; loss(y_pred,y_true)
</code></pre>
<p>Args:
y_true: Ground truth values. shape = <code>[batch_size, d0, .. dN]</code>.
y_pred: The predicted values. shape = <code>[batch_size, d0, .. dN]</code>.
Returns:
Mean absolute error values. shape = <code>[batch_size, d0, .. dN-1]</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MAE(Loss):
    &#34;&#34;&#34;Mean Absolute Error.

    Computes the mean absolute error between labels and predictions.

    `loss = mean(abs(y_true - y_pred), axis=-1)`

    Standalone usage:
    ```python
    &gt;&gt;&gt; y_true = np.random.randint(0, 2, size=(2, 3))
    &gt;&gt;&gt; y_pred = np.random.random(size=(2, 3))
    &gt;&gt;&gt; loss = asf.losses.MAE()
    &gt;&gt;&gt; loss(y_pred,y_true)
    ```
     Args:
        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.
        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.
     Returns:
        Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`.
    &#34;&#34;&#34;
    __name__ = &#39;MAE&#39;

    def __call__(self, y_pred, y_true):
        m = y_true.shape[0]
        return np.sum(np.abs(y_pred - y_true)) / m

    def diff(self, y_pred, y_true):
        m = y_true.shape[0]
        return np.sign(y_pred - y_true) / m</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ainshamsflow.losses.MAPE"><code class="flex name class">
<span>class <span class="ident">MAPE</span></span>
</code></dt>
<dd>
<div class="desc"><p>Mean absolute percentage error.</p>
<p>Computes the mean absolute percentage error between <code>y_true</code> and <code>y_pred</code>.</p>
<p><code>loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)</code>
Standalone usage:</p>
<pre><code class="python">&gt;&gt;&gt; y_true = np.random.rand(2, 3)
&gt;&gt;&gt; y_pred = np.random.random(size=(2, 3))
&gt;&gt;&gt; loss = asf.losses.MAPE()
&gt;&gt;&gt; loss(y_pred,y_true)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y_true</code></strong></dt>
<dd>Ground truth values. shape = <code>[batch_size, d0, .. dN]</code>.</dd>
<dt><strong><code>y_pred</code></strong></dt>
<dd>The predicted values. shape = <code>[batch_size, d0, .. dN]</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Mean absolute percentage error values. shape = <code>[batch_size, d0, .. dN-1]</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MAPE(Loss):
    &#34;&#34;&#34;Mean absolute percentage error.

    Computes the mean absolute percentage error between `y_true` and `y_pred`.

    `loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)`
    Standalone usage:
    ```python
    &gt;&gt;&gt; y_true = np.random.rand(2, 3)
    &gt;&gt;&gt; y_pred = np.random.random(size=(2, 3))
    &gt;&gt;&gt; loss = asf.losses.MAPE()
    &gt;&gt;&gt; loss(y_pred,y_true)
    ```
    Args:
        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.
        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.
    Returns:
        Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`.
    &#34;&#34;&#34;
    __name__ = &#39;MAPE&#39;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return np.sum(np.abs(y_pred - y_true) / y_true) / m

    def diff(self, y_pred, y_true):
        m = y_true.shape[0]
        return np.where(y_pred &gt; y_true, 1 / (m * y_true), -1 / (m * y_true))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ainshamsflow.losses.MSE"><code class="flex name class">
<span>class <span class="ident">MSE</span></span>
</code></dt>
<dd>
<div class="desc"><p>Mean squared error loss function.</p>
<p>Computes the mean squared error between labels and predictions.</p>
<p>After computing the squared distance between the inputs, the mean value over
the last dimension is returned.
<code>loss = mean(square(y_true - y_pred), axis=-1)</code></p>
<p>Standalone usage:</p>
<pre><code class="python">&gt;&gt;&gt; y_true = np.random.randint(0, 2, size=(2, 3))
&gt;&gt;&gt; y_pred = np.random.random(size=(2, 3))
&gt;&gt;&gt; loss = asf.losses.MSE()
&gt;&gt;&gt; loss(y_pred,y_true)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y_true</code></strong></dt>
<dd>Ground truth values. shape = <code>[batch_size, d0, .. dN]</code>.</dd>
<dt><strong><code>y_pred</code></strong></dt>
<dd>The predicted values. shape = <code>[batch_size, d0, .. dN]</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>error_values</code></dt>
<dd>Mean squared error values. shape = <code>[batch_size, d0, .. dN-1]</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MSE(Loss):
    &#34;&#34;&#34;Mean squared error loss function.

    Computes the mean squared error between labels and predictions.

    After computing the squared distance between the inputs, the mean value over
    the last dimension is returned.
    `loss = mean(square(y_true - y_pred), axis=-1)`

    Standalone usage:

    ```python
    &gt;&gt;&gt; y_true = np.random.randint(0, 2, size=(2, 3))
    &gt;&gt;&gt; y_pred = np.random.random(size=(2, 3))
    &gt;&gt;&gt; loss = asf.losses.MSE()
    &gt;&gt;&gt; loss(y_pred,y_true)
    ```
    Args:
        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.
        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.
    Returns:
        error_values: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`.
    &#34;&#34;&#34;
    __name__ = &#39;MSE&#39;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        return np.sum(np.square(y_pred - y_true)) / (2 * m)

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        return (y_pred - y_true) / m</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ainshamsflow.losses.PerceptronCriterion"><code class="flex name class">
<span>class <span class="ident">PerceptronCriterion</span></span>
</code></dt>
<dd>
<div class="desc"><p>Bipolar perceptron criterion loss class.
Standalone usage:</p>
<pre><code class="python">&gt;&gt;&gt; y_pred=2
&gt;&gt;&gt; y_true=2.4
&gt;&gt;&gt; loss=asf.losses.PerceptronCriterion()
&gt;&gt;&gt; loss(y_pred,y_true)
0
&gt;&gt;&gt; loss.(-y_pred,y_true)
4.8
</code></pre>
<h2 id="returns">Returns</h2>
<p>0 if both numbers are positives or negatives
otherwise ,returns their product</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PerceptronCriterion(Loss):
    &#34;&#34;&#34;Bipolar perceptron criterion loss class.
       Standalone usage:
       ```python
       &gt;&gt;&gt; y_pred=2
       &gt;&gt;&gt; y_true=2.4
       &gt;&gt;&gt; loss=asf.losses.PerceptronCriterion()
       &gt;&gt;&gt; loss(y_pred,y_true)
       0
       &gt;&gt;&gt; loss.(-y_pred,y_true)
       4.8
       ```
       Returns:
           0 if both numbers are positives or negatives
           otherwise ,returns their product
    &#34;&#34;&#34;
    __name__ = &#39;PerceptronCriterion&#39;

    def __call__(self, y_pred, y_true):
        return np.maximum(0, -y_true * y_pred)

    def diff(self, y_pred, y_true):
        return np.where(y_true * y_pred &lt;= 0, -y_true, 0)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ainshamsflow.losses.SparseCategoricalCrossentropy"><code class="flex name class">
<span>class <span class="ident">SparseCategoricalCrossentropy</span></span>
</code></dt>
<dd>
<div class="desc"><p>Computes the crossentropy loss between the labels and predictions.</p>
<p>Use this crossentropy loss function when there are two or more label classes.
We expect labels to be provided as integers. If you want to provide labels
using <code>one-hot</code> representation, please use <code><a title="ainshamsflow.losses.CategoricalCrossentropy" href="#ainshamsflow.losses.CategoricalCrossentropy">CategoricalCrossentropy</a></code> loss.
There should be <code># classes</code> floating point values per feature for <code>y_pred</code>
and a single floating point value per feature for <code>y_true</code></p>
<p>Standalone usage:</p>
<pre><code class="python">&gt;&gt;&gt;  y_true = [[1], [2]]
&gt;&gt;&gt;  y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
&gt;&gt;&gt;  loss = asf.losses.SparseCategoricalCrossentropy)
&gt;&gt;&gt;  loss(y_pred,y_true)
1.177
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SparseCategoricalCrossentropy(Loss):
    &#34;&#34;&#34;Computes the crossentropy loss between the labels and predictions.

      Use this crossentropy loss function when there are two or more label classes.
      We expect labels to be provided as integers. If you want to provide labels
      using `one-hot` representation, please use `CategoricalCrossentropy` loss.
      There should be `# classes` floating point values per feature for `y_pred`
      and a single floating point value per feature for `y_true`

      Standalone usage:
      ```python
      &gt;&gt;&gt;  y_true = [[1], [2]]
      &gt;&gt;&gt;  y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
      &gt;&gt;&gt;  loss = asf.losses.SparseCategoricalCrossentropy)
      &gt;&gt;&gt;  loss(y_pred,y_true)
      1.177
      ```
    &#34;&#34;&#34;
    __name__ = &#39;SparseCategoricalCrossentropy&#39;

    def __call__(self, y_pred, y_true):
        m = y_pred.shape[0]
        n_c = y_pred.shape[-1]
        y_true = true_one_hot(y_true, n_c)
        return -np.sum(np.log(np.max(y_true * y_pred, axis=1))) / m

    def diff(self, y_pred, y_true):
        m = y_pred.shape[0]
        n_c = y_pred.shape[-1]
        y_true = true_one_hot(y_true, n_c)
        return (y_pred - y_true) / m</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ainshamsflow.losses.SvmHingeLoss"><code class="flex name class">
<span>class <span class="ident">SvmHingeLoss</span></span>
</code></dt>
<dd>
<div class="desc"><p>SVM hinge criterion loss.</p>
<p>Stand alone usage:</p>
<pre><code class="python">&gt;&gt;&gt; y_pred=2
&gt;&gt;&gt; y_true=2.4
&gt;&gt;&gt; loss=asf.losses.SvmHingeLoss()
&gt;&gt;&gt; loss(y_pred,y_true)
0
&gt;&gt;&gt; loss(-y_pred,y_true)
5.8
</code></pre>
<p>Returns:
0 if product of the two numbers is greater than 1
otherwise ,returns 1-their product</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SvmHingeLoss(Loss):
    &#34;&#34;&#34;SVM hinge criterion loss.

    Stand alone usage:
    ```python
    &gt;&gt;&gt; y_pred=2
    &gt;&gt;&gt; y_true=2.4
    &gt;&gt;&gt; loss=asf.losses.SvmHingeLoss()
    &gt;&gt;&gt; loss(y_pred,y_true)
    0
    &gt;&gt;&gt; loss(-y_pred,y_true)
    5.8
    ```
     Returns:
        0 if product of the two numbers is greater than 1
        otherwise ,returns 1-their product
    &#34;&#34;&#34;
    __name__ = &#39;SvmHingeLoss&#39;

    def __call__(self, y_pred, y_true):
        return np.maximum(0, 1 - y_true * y_pred)

    def diff(self, y_pred, y_true):
        return np.where(y_true * y_pred &lt;= 1, -y_true, 0)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></b></code>:
<ul class="hlist">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ainshamsflow" href="index.html">ainshamsflow</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ainshamsflow.losses.get" href="#ainshamsflow.losses.get">get</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ainshamsflow.losses.BinaryCrossentropy" href="#ainshamsflow.losses.BinaryCrossentropy">BinaryCrossentropy</a></code></h4>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.CategoricalCrossentropy" href="#ainshamsflow.losses.CategoricalCrossentropy">CategoricalCrossentropy</a></code></h4>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.HuberLoss" href="#ainshamsflow.losses.HuberLoss">HuberLoss</a></code></h4>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.LogLossLinear" href="#ainshamsflow.losses.LogLossLinear">LogLossLinear</a></code></h4>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.LogLossSigmoid" href="#ainshamsflow.losses.LogLossSigmoid">LogLossSigmoid</a></code></h4>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.Loss" href="#ainshamsflow.losses.Loss">Loss</a></code></h4>
<ul class="">
<li><code><a title="ainshamsflow.losses.Loss.__call__" href="#ainshamsflow.losses.Loss.__call__">__call__</a></code></li>
<li><code><a title="ainshamsflow.losses.Loss.diff" href="#ainshamsflow.losses.Loss.diff">diff</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.MAE" href="#ainshamsflow.losses.MAE">MAE</a></code></h4>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.MAPE" href="#ainshamsflow.losses.MAPE">MAPE</a></code></h4>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.MSE" href="#ainshamsflow.losses.MSE">MSE</a></code></h4>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.PerceptronCriterion" href="#ainshamsflow.losses.PerceptronCriterion">PerceptronCriterion</a></code></h4>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.SparseCategoricalCrossentropy" href="#ainshamsflow.losses.SparseCategoricalCrossentropy">SparseCategoricalCrossentropy</a></code></h4>
</li>
<li>
<h4><code><a title="ainshamsflow.losses.SvmHingeLoss" href="#ainshamsflow.losses.SvmHingeLoss">SvmHingeLoss</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>